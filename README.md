# Awesome NeurIPS 2025 Vision-Language Models (VLMs) üöÄ
A curated collection of **Vision-Language Model (VLM)** papers accepted at NeurIPS 2025 with GitHub links.

---

## VLM Architecture & Training

| Paper | Links |
|-------|-------|
| **FastVLM: Efficient Vision Encoding for Vision Language Models** | [Paper](https://arxiv.org/abs/2412.13303) ¬∑ [GitHub](https://github.com/apple/ml-fastvlm) |
| **NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding** | [GitHub](https://github.com/H-EmbodVis/NAUTILUS) |
| **VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning** | [Paper](https://arxiv.org/abs/2507.13348) ¬∑ [GitHub](https://github.com/dvlab-research/VisionThink) |
| **VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning** | [Paper](https://arxiv.org/abs/2509.25033) ¬∑ [GitHub](https://github.com/peacelwh/VT-FSL) |
| **On the Value of Cross-Modal Misalignment in Multimodal Representation Learning** | [GitHub](https://github.com/YichaoCai1/crossmodal_misalignment) |
| **VL-SAE: Interpreting and Enhancing Vision-Language Alignment** | [Paper](https://arxiv.org/abs/2510.21323) ¬∑ [GitHub](https://github.com/ssfgunner/VL-SAE) |
| **UniTok: A Unified Tokenizer for Visual Generation and Understanding** ‚≠ê Spotlight | [Paper](https://arxiv.org/abs/2502.20321) ¬∑ [GitHub](https://github.com/FoundationVision/UniTok) |
| **VisPer-LM: Elevating Visual Perception in Multimodal LLMs** | [GitHub](https://github.com/SHI-Labs/VisPer-LM) |
| **VFMTok: Vision Foundation Models as Effective Visual Tokenizers** | [Paper](https://arxiv.org/abs/2507.08441) ¬∑ [GitHub](https://github.com/CVMI-Lab/VFMTok) |

---

## Video Understanding & Video-LLMs

| Paper | Links |
|-------|-------|
| **VideoChat-R1 & R1.5: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning** | [Paper](https://arxiv.org/abs/2504.06958) ¬∑ [GitHub](https://github.com/OpenGVLab/VideoChat-R1) |
| **Eagle 2.5: Boosting Long-Context Post-Training for Frontier VLMs** | [Paper](https://arxiv.org/abs/2504.15271) ¬∑ [GitHub](https://github.com/NVlabs/Eagle) |
| **StreamForest: Efficient Online Video Understanding with Persistent Event Memory** ‚≠ê Spotlight | [Paper](https://arxiv.org/abs/2509.24871) ¬∑ [GitHub](https://github.com/MCG-NJU/StreamForest) |
| **VideoRFT: Incentivizing Video Reasoning Capability in MLLMs** | [Paper](https://arxiv.org/abs/2505.12434) ¬∑ [GitHub](https://github.com/QiWang98/VideoRFT) |
| **Video-R1: Reinforcing Video Reasoning in MLLMs** | [GitHub](https://github.com/tulerfeng/Video-R1) |
| **CameraBench: Towards Understanding Camera Motions in Any Video** ‚≠ê Spotlight | [GitHub](https://github.com/sy77777en/CameraBench) |
| **Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views** ‚≠ê Spotlight | [GitHub](https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding) |
| **VQToken: Neural Discrete Token Representation Learning for Video LLMs** | [GitHub](https://github.com/Hai-chao-Zhang/VQToken) |

---

## Multimodal Reasoning & Chain-of-Thought

| Paper | Links |
|-------|-------|
| **LaCoT: Latent Chain-of-Thought for Visual Reasoning** | [GitHub](https://github.com/heliossun/LaCoT) |
| **BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset** | [Paper](https://arxiv.org/abs/2507.03483) ¬∑ [GitHub](https://github.com/WooooDyy/BMMR) |

---

## Benchmarks & Evaluation

| Paper | Links |
|-------|-------|
| **MME: A Comprehensive Evaluation Benchmark for MLLMs** ‚≠ê DB Highlight | [GitHub](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) |
| **OCRBench v2: Evaluating LMMs on Visual Text Localization and Reasoning** | [GitHub](https://github.com/Yuliang-Liu/MultimodalOCR) ¬∑ [Project](https://99franklin.github.io/ocrbench_v2/) |
| **Situat3DChange: A 3D Visual-Language Benchmark for Real-World Change Understanding** | [GitHub](https://github.com/RuipingL/Situat3DChange) |
| **OpenCaptchaWorld: Testing Multimodal LLM Agents via CAPTCHA Puzzles** | [GitHub](https://github.com/MetaAgentX/OpenCaptchaWorld) |
| **SURDS: Benchmarking Spatial Understanding in Driving Scenarios** | [GitHub](https://github.com/XiandaGuo/Drive-MLLM) |
| **V2X-Radar: A Multi-modal Dataset with 4D Radar** ‚≠ê Spotlight | [GitHub](https://github.com/yanglei18/V2X-Radar) |
| **OpenS2V-Nexus: A Benchmark for Subject-to-Video Generation** | [GitHub](https://github.com/PKU-YuanGroup/OpenS2V-Nexus) |
| **EyeBench: Predictive Modeling from Eye Movements in Reading** | [Project](https://eyebench.github.io/) |

---

## Efficient VLMs & Compression

| Paper | Links |
|-------|-------|
| **AutoPrune: Complexity-Adaptive Pruning for VLMs** | [GitHub](https://github.com/AutoLab-SAI-SJTU/AutoPrune) |
| **CDPruner: Maximizing Conditional Diversity for Token Pruning in MLLMs** | [Paper](https://arxiv.org/abs/2506.10967) ¬∑ [GitHub](https://github.com/Theia-4869/CDPruner) |
| **EAGLE-3: Scaling up Inference Acceleration of LLMs** | [GitHub](https://github.com/SafeAILab/EAGLE) |
| **VLA-Cache: Efficient VLA Model via Adaptive Token Caching** | [Paper](https://arxiv.org/abs/2502.02175) ¬∑ [GitHub](https://github.com/siyuhsu/vla-cache) |

---

## Embodied AI & Robotics

| Paper | Links |
|-------|-------|
| **OpenCUA: Open Foundations for Computer-Use Agents** ‚≠ê Spotlight | [Paper](https://arxiv.org/abs/2508.09123) ¬∑ [GitHub](https://github.com/xlang-ai/OpenCUA) ¬∑ [Project](https://opencua.xlang.ai/) |
| **CORE: Reducing UI Exposure in Mobile Agents** | [GitHub](https://github.com/Entropy-Fighter/CORE) |

---

## Medical & Scientific VLMs

| Paper | Links |
|-------|-------|
| **ExGra-Med: Medical Multi-Modal LLM with Extended Context Alignment** | [Paper](https://arxiv.org/abs/2410.02615) ¬∑ [GitHub](https://github.com/duyhominhnguyen/Exgra-Med) |
| **HiVE-MIL: Few-Shot Learning from Gigapixel Images via Hierarchical VL Alignment** | [Paper](https://arxiv.org/abs/2505.17982) ¬∑ [GitHub](https://github.com/bryanwong17/HiVE-MIL) |

---

## Hallucination & Trustworthiness

| Paper | Links |
|-------|-------|
| **REVERSE: Reducing Hallucination in VLMs with Retrospective Resampling** | [Paper](https://arxiv.org/abs/2504.13169) ¬∑ [GitHub](https://github.com/tsunghan-wu/reverse_vlm) ¬∑ [Project](https://reverse-vlm.github.io/) |
| **Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations** | [GitHub](https://github.com/SooLab/AllPath) |
| **HaMI: Robust Hallucination Detection in LLMs via Adaptive Token Selection** | [Paper](https://arxiv.org/abs/2504.07863) ¬∑ [GitHub](https://github.com/mala-lab/HaMI) |
